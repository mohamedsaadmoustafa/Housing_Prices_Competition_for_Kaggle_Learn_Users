{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n","metadata":{"papermill":{"duration":0.068693,"end_time":"2022-01-21T16:18:28.537628","exception":false,"start_time":"2022-01-21T16:18:28.468935","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Imports and Configuration\nWe'll Start by importing the packages we use and setting some notebook defaults. ","metadata":{"papermill":{"duration":0.063274,"end_time":"2022-01-21T16:18:28.664952","exception":false,"start_time":"2022-01-21T16:18:28.601678","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom tqdm.notebook import tqdm\nimport warnings\nimport random\n\nfrom scipy import stats     # norm, skew, kurtosis, boxcox\nfrom scipy import special  # boxcox1p, inv_boxcox, inv_boxcox1p\n\n### sklearn \nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso, LogisticRegression,RidgeCV, LassoLars, BayesianRidge, SGDRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, KFold, cross_validate\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, PowerTransformer, StandardScaler, PolynomialFeatures, scale\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\n# xgboost\nfrom xgboost import XGBRegressor","metadata":{"papermill":{"duration":1.693695,"end_time":"2022-01-21T16:18:30.422608","exception":false,"start_time":"2022-01-21T16:18:28.728913","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:24:53.391845Z","iopub.execute_input":"2022-01-29T20:24:53.392949Z","iopub.status.idle":"2022-01-29T20:24:53.405196Z","shell.execute_reply.started":"2022-01-29T20:24:53.392891Z","shell.execute_reply":"2022-01-29T20:24:53.404374Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nnp.random.seed(42)\n\ntqdm().pandas()\npd.set_option('display.max_colwidth', None)\npd.set_option('use_inf_as_na', True)\nsns.set_style( 'whitegrid' )\n\n# Set Matplotlib defaults\n#plt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')\n\n#Limiting floats output to 3 decimal points\npd.set_option(\n    'display.float_format',\n    lambda x: '{:.3f}'.format(x)\n) ","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:02:00.706819Z","iopub.execute_input":"2022-01-29T20:02:00.707248Z","iopub.status.idle":"2022-01-29T20:02:00.746650Z","shell.execute_reply.started":"2022-01-29T20:02:00.707195Z","shell.execute_reply":"2022-01-29T20:02:00.746077Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Set up code checking\nos.symlink() method is used to create symbolic link. This method creates symbolic link pointing to source named destination.","metadata":{"papermill":{"duration":0.062645,"end_time":"2022-01-21T16:18:30.548509","exception":false,"start_time":"2022-01-21T16:18:30.485864","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") ","metadata":{"papermill":{"duration":0.075827,"end_time":"2022-01-21T16:18:30.68716","exception":false,"start_time":"2022-01-21T16:18:30.611333","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:02:00.747803Z","iopub.execute_input":"2022-01-29T20:02:00.748047Z","iopub.status.idle":"2022-01-29T20:02:00.753708Z","shell.execute_reply.started":"2022-01-29T20:02:00.748018Z","shell.execute_reply":"2022-01-29T20:02:00.752366Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Fast Run!","metadata":{"papermill":{"duration":0.062743,"end_time":"2022-01-21T16:18:30.813863","exception":false,"start_time":"2022-01-21T16:18:30.75112","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\n# Remove rows with missing target, separate target from predictors\ndf_train = pd.read_csv(\"../input/train.csv\", index_col=0)\ndf_test = pd.read_csv(\"../input/test.csv\", index_col=0)\n\nx = df_train.dropna(axis=0, subset=['SalePrice'])\ny = df_train.SalePrice\nx = df_train.drop(['SalePrice'], axis=1)\n\n# Select numerical columns\ncategorical_features = x.select_dtypes(include=np.object).columns.tolist()\nnumerical_features = x.select_dtypes(include='number').columns.tolist()\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\npipeline = Pipeline(steps=[\n    ('preprocess', preprocessor),\n    ('model', GradientBoostingRegressor(n_estimators=200, learning_rate=0.09))\n])\n\npipeline.fit(x, y)\n\nSave_Submit(pipeline, df_test)\n\"\"\";","metadata":{"papermill":{"duration":0.072869,"end_time":"2022-01-21T16:18:30.949743","exception":false,"start_time":"2022-01-21T16:18:30.876874","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:02:00.755888Z","iopub.execute_input":"2022-01-29T20:02:00.756401Z","iopub.status.idle":"2022-01-29T20:02:00.769373Z","shell.execute_reply.started":"2022-01-29T20:02:00.756359Z","shell.execute_reply":"2022-01-29T20:02:00.768499Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Let's read our train dataset.","metadata":{"papermill":{"duration":0.063657,"end_time":"2022-01-21T16:18:31.078017","exception":false,"start_time":"2022-01-21T16:18:31.01436","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Read the data\ndf = pd.read_csv(\n    \"../input/train.csv\", \n    index_col=0\n)\n#df.reset_index(drop=True, inplace=True)\ndf","metadata":{"papermill":{"duration":0.163759,"end_time":"2022-01-21T16:18:31.304973","exception":false,"start_time":"2022-01-21T16:18:31.141214","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:02:00.772166Z","iopub.execute_input":"2022-01-29T20:02:00.772539Z","iopub.status.idle":"2022-01-29T20:02:00.855837Z","shell.execute_reply.started":"2022-01-29T20:02:00.772494Z","shell.execute_reply":"2022-01-29T20:02:00.855272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_description = {\n    'SalePrice': 'sale price in dollars (our target)',\n    'MSSubClass': 'The building class',\n    'MSZoning': 'The general zoning classification',\n    'LotFrontage': 'Linear feet of street connected to property',\n    'LotArea': 'Lot size in square feet',\n    'Street': 'Type of road access',\n    'Alley': 'Type of alley access',\n    'LotShape': 'General shape of property',\n    'LandContour': 'Flatness of the property',\n    'Utilities': 'Type of utilities available',\n    'LotConfig': 'Lot configuration',\n    'LandSlope': 'Slope of property',\n    'Neighborhood': 'Physical locations within Ames city limits',\n    'Condition1': 'Proximity to main road or railroad',\n    'Condition2': 'Proximity to main road or railroad (if a second is present)',\n    'BldgType': 'Type of dwelling',\n    'HouseStyle': 'Style of dwelling',\n    'OverallQual': 'Overall material and finish quality',\n    'OverallCond': 'Overall condition rating',\n    'YearBuilt': 'Original construction date',\n    'YearRemodAdd': 'Remodel date',\n    'RoofStyle': 'Type of roof',\n    'RoofMatl': 'Roof material',\n    'Exterior1st': 'Exterior covering on house',\n    'Exterior2nd': 'Exterior covering on house (if more than one material)',\n    'MasVnrType': 'Masonry veneer type',\n    'MasVnrArea': 'Masonry veneer area in square feet',\n    'ExterQual': 'Exterior material quality',\n    'ExterCond': 'Present condition of the material on the exterior',\n    'Foundation': 'Type of foundation',\n    'BsmtQual': 'Height of the basement',\n    'BsmtCond': 'General condition of the basement',\n    'BsmtExposure': 'Walkout or garden level basement walls',\n    'BsmtFinType1': 'Quality of basement finished area',\n    'BsmtFinSF1': 'Type 1 finished square feet',\n    'BsmtFinType2': 'Quality of second finished area (if present)',\n    'BsmtFinSF2': 'Type 2 finished square feet',\n    'BsmtUnfSF': 'Unfinished square feet of basement area',\n    'TotalBsmtSF': 'Total square feet of basement area',\n    'Heating': 'Type of heating',\n    'HeatingQC': 'Heating quality and condition',\n    'CentralAir': 'Central air conditioning',\n    'Electrical': 'Electrical system',\n    '1stFlrSF': 'First Floor square feet',\n    '2ndFlrSF': 'Second floor square feet',\n    'LowQualFinSF': 'Low quality finished square feet (all floors)',\n    'GrLivArea': 'Above grade (ground) living area square feet',\n    'BsmtFullBath': 'Basement full bathrooms',\n    'BsmtHalfBath': 'Basement half bathrooms',\n    'FullBath': 'Full bathrooms above grade',\n    'HalfBath': 'Half baths above grade',\n    'Bedroom': 'Number of bedrooms above basement level',\n    'Kitchen': 'Number of kitchens',\n    'KitchenQual': 'Kitchen quality',\n    'TotRmsAbvGrd': 'Total rooms above grade (does not include bathrooms)',\n    'Functional': 'Home functionality rating',\n    'Fireplaces': 'Number of fireplaces',\n    'FireplaceQu': 'Fireplace quality',\n    'GarageType': 'Garage location',\n    'GarageYrBlt': 'Year garage was built',\n    'GarageFinish': 'Interior finish of the garage',\n    'GarageCars': 'Size of garage in car capacity',\n    'GarageArea': 'Size of garage in square feet',\n    'GarageQual': 'Garage quality',\n    'GarageCond': 'Garage condition',\n    'PavedDrive': 'Paved driveway',\n    'WoodDeckSF': 'Wood deck area in square feet',\n    'OpenPorchSF': 'Open porch area in square feet',\n    'EnclosedPorch': 'Enclosed porch area in square feet',\n    '3SsnPorch': 'Three season porch area in square feet',\n    'ScreenPorch': 'Screen porch area in square feet',\n    'PoolArea': 'Pool area in square feet',\n    'PoolQC': 'Pool quality',\n    'Fence': 'Fence quality',\n    'MiscFeature': 'Miscellaneous feature not covered in other categories',\n    'MiscVal': '$Value of miscellaneous feature',\n    'MoSold': 'Month Sold',\n    'YrSold': 'Year Sold',\n    'SaleType': 'Type of sale',\n    'SaleCondition': 'Condition of sale',\n}","metadata":{"papermill":{"duration":0.081444,"end_time":"2022-01-21T16:18:31.450029","exception":false,"start_time":"2022-01-21T16:18:31.368585","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:02:00.856907Z","iopub.execute_input":"2022-01-29T20:02:00.857241Z","iopub.status.idle":"2022-01-29T20:02:00.867665Z","shell.execute_reply.started":"2022-01-29T20:02:00.857198Z","shell.execute_reply":"2022-01-29T20:02:00.866681Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def explore_target( y = df['SalePrice'] ):\n    skew_y = stats.skew( y )\n    # kurtosis a measure of the combined weight \n    # of a distribution's tails relative to the center of the distribution.\n    kurtosis_y = stats.kurtosis( y )\n    mu, sigma = stats.norm.fit( y )\n    print( f\"y: mu = {mu:.2f} and sigma = {sigma:.2f}, skew = {skew_y:.2f} kurtosis = {kurtosis_y:.2f}\")\n    #------------\n    #log_y = np.log( y )\n    log_y = np.log1p( y )\n    log_skew_y = stats.skew( log_y )\n    log_kurtosis_y = stats.kurtosis( log_y )\n    log_mu, log_sigma = stats.norm.fit( log_y )\n    print( f\"Log y: mu = {log_mu:.2f} and sigma = {log_sigma:.2f}, skew = {log_skew_y:.2f} kurtosis = {log_kurtosis_y:.2f}\")\n    #------------\n    # A Box Cox transformation is a transformation of non-normal dependent variables \n    # into a normal shape.\n    #boxcox_y = stats.boxcox( y )[0]\n    # try different alpha values \"optimized value\"  between 0 and 1\n    boxcox_y = special.boxcox1p(y, 0.35)\n    boxcox_skew_y = stats.skew( boxcox_y )\n    boxcox_kurtosis_y = stats.kurtosis( boxcox_y )\n    boxcox_mu, boxcox_sigma = stats.norm.fit( boxcox_y )\n    print( f\"boxcox y: mu = {boxcox_mu:.2f} and sigma = {boxcox_sigma:.2f}, skew = {boxcox_skew_y:.2f} kurtosis = {boxcox_kurtosis_y:.2f}\")\n\n    # graph\n    fig, axes = plt.subplots( 3, 2, figsize = ( 14, 10 ) );\n    \n    sns.distplot(y, fit=stats.norm, ax=axes[0,0])\n    axes[0,0].set_xlabel('SalePrice distribution');\n    axes[0,0].set_title( 'Probplot against normal distribution' );\n    axes[0,0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n\n    prob1 = stats.probplot( y, plot = axes[0,1] );\n    axes[0,1].set_xlabel('');\n    axes[0,1].set_title( 'probability plot' );\n    #-----------------------------------------------------\n    sns.distplot(log_y, fit=stats.norm, ax=axes[1,0])\n    axes[1,0].set_xlabel('Log SalePrice distribution');\n    axes[1,0].set_title( 'Probplot against normal distribution' );\n    axes[1,0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(log_mu, log_sigma)], loc='best')\n\n    prob2 = stats.probplot( log_y, plot = axes[1,1] );\n    axes[1,1].set_title( 'probability plot after log transform' );\n    #-----------------------------------------------------\n    sns.distplot(boxcox_y, fit=stats.norm, ax=axes[2,0])\n    axes[2,0].set_xlabel('boxcox SalePrice distribution');\n    axes[2,0].set_title( 'Probplot against normal distribution' );\n    axes[2,0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(boxcox_mu, boxcox_sigma)], loc='best')\n\n    prob3 = stats.probplot( boxcox_y, plot = axes[2,1] );\n    axes[2,1].set_title( 'probability plot after boxcox transform' );\n\nexplore_target(df['SalePrice'])","metadata":{"papermill":{"duration":0.081812,"end_time":"2022-01-21T16:18:31.732773","exception":false,"start_time":"2022-01-21T16:18:31.650961","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:02:00.869116Z","iopub.execute_input":"2022-01-29T20:02:00.869395Z","iopub.status.idle":"2022-01-29T20:02:03.535168Z","shell.execute_reply.started":"2022-01-29T20:02:00.869362Z","shell.execute_reply":"2022-01-29T20:02:03.534269Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def explore_feature(x, y=df['SalePrice']):\n    #log_y = np.log(y)\n    log_y = np.log1p(y)\n    \n    # regression\n    m, b = np.polyfit(x, log_y, 1) # m = slope, b = intercept\n    reg = m*x+b\n     \n    # let's try polynomial fit  \"linear\" \n    curve_fit = np.polyfit(x, log_y, 1)\n    print(f\"curve fit 2: {curve_fit}\")\n    y_poly_1 = np.exp(curve_fit[1]) * np.exp(curve_fit[0]*x)\n    \n    # let's try polynomial fit l\"inear with log y\"\n    curve_fit_gla = np.polyfit(x, y, 2)\n    print(f\"curve fit 2: {curve_fit_gla}\")\n    y_poly_2 = curve_fit_gla[2] + curve_fit_gla[1]*x + curve_fit_gla[0]*x**2\n    \n    # graph\n    fig, axes = plt.subplots( 3, 3, figsize = ( 14, 10 ) );\n    # -----------------------------------------------------------------\n    sns.regplot(x=x, y=y, marker=\"+\", ax=axes[0,0] )\n    axes[0,0].set_ylabel(y.name, fontsize=13)\n    axes[0,0].set_xlabel(x.name, fontsize=13)\n    axes[0,0].set_title( 'Regression Line' );\n    \n    sns.regplot(x=x, y=log_y, marker=\"+\", ax=axes[0,1] )\n    axes[0,1].set_ylabel(y.name, fontsize=13)\n    axes[0,1].set_xlabel(x.name, fontsize=13)\n    axes[0,1].set_title( 'Regression Line  with log y' );\n    \n    axes[0,2].scatter(x, y_poly_1, c='red')\n    axes[0,2].plot(x, y, \"+\")\n    axes[0,2].set_ylabel(y.name, fontsize=13)\n    axes[0,2].set_xlabel(x.name, fontsize=13)\n    axes[0,2].set_title( \"polynomial fit  'linear' \" );\n    # -----------------------------------------------------------------\n    axes[1,0].scatter(x, np.log(y_poly_1), c='red')\n    axes[1,0].plot(x, log_y, \"+\")\n    axes[1,0].set_ylabel(f\"Log {y.name}\", fontsize=13)\n    axes[1,0].set_xlabel(x.name, fontsize=13)\n    axes[1,0].set_title( \"polynomial fit  'linear' with log y\" );\n\n    axes[1,1].scatter(x, y_poly_2, c='red')\n    axes[1,1].plot(x, y, \"+\")\n    axes[1,1].set_ylabel(y.name, fontsize=13)\n    axes[1,1].set_xlabel(x.name, fontsize=13)\n    axes[1,1].set_title( \"polynomial fit \" );\n    \n    axes[1,2].scatter(x, np.log(y_poly_2), c='red')\n    axes[1,2].plot(x, log_y, \"+\")\n    axes[1,2].set_ylabel(f\"Log {y.name}\", fontsize=13)\n    axes[1,2].set_xlabel(x.name, fontsize=13)\n    axes[1,2].set_title( \"polynomial fit  with log y\" );\n    # -----------------------------------------------------------------\n    sns.boxplot(x=x, y=y, ax=axes[2,0] )\n    axes[2,0].set_xlabel( f\"{x.name}\" );\n    axes[2,0].set_title( 'box plot' );\n    \n    sns.boxplot(x=x, y=log_y, ax=axes[2,1] )\n    axes[2,1].set_xlabel( f\"{x.name}\" );\n    axes[2,1].set_title( 'box plot' );\n    \n    sns.histplot(x=x, kde=True, ax=axes[2,2] )\n    axes[2,2].set_xlabel( f\"{x.name}\" );\n    axes[2,2].set_title( 'hist plot' );\n    \nexplore_feature(df.GrLivArea, df.SalePrice)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:02:03.537197Z","iopub.execute_input":"2022-01-29T20:02:03.537859Z","iopub.status.idle":"2022-01-29T20:03:19.666491Z","shell.execute_reply.started":"2022-01-29T20:02:03.537814Z","shell.execute_reply":"2022-01-29T20:03:19.665607Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Let's create def r_squared function","metadata":{}},{"cell_type":"code","source":"def r_squared(x, y):\n    return np.corrcoef(x, y)[0,1]**2","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:19.667578Z","iopub.execute_input":"2022-01-29T20:03:19.667977Z","iopub.status.idle":"2022-01-29T20:03:19.672875Z","shell.execute_reply.started":"2022-01-29T20:03:19.667944Z","shell.execute_reply":"2022-01-29T20:03:19.672057Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def gla_feature(x, y):\n    def add_gla(row, x=x, y=y):\n        p = np.polyfit(x, y, 2)\n        return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2))\n    return df.apply(lambda row: add_gla(row), axis=1)\n\n#r_squared( df.GrLivArea, df.SalePrice), r_squared( a, df.SalePrice), ","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:19.675481Z","iopub.execute_input":"2022-01-29T20:03:19.675732Z","iopub.status.idle":"2022-01-29T20:03:19.685325Z","shell.execute_reply.started":"2022-01-29T20:03:19.675700Z","shell.execute_reply":"2022-01-29T20:03:19.684594Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"###  value that shows up only once in the dataset\ndelete values since the it is guaranteed to create a std of zero. DON'T apply this function in test dataset.","metadata":{"papermill":{"duration":0.063317,"end_time":"2022-01-21T16:18:31.860243","exception":false,"start_time":"2022-01-21T16:18:31.796926","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def only_once_values(df):\n    X = df.copy()\n    n_rows = X.shape[0]\n    only_once = {}\n    for name in X.select_dtypes(exclude=['number']):\n        once = X[name].value_counts() <= 1\n        if (once).any():\n            #print(f\"\\n========={name}=============\")\n            #print(X[name].value_counts())\n            # store items that that shows up only once\n            only_once[name] = once[once].index.tolist()\n            for i in once[once].index.tolist():\n                X = X[X[name] != i]\n    #print(only_once)\n    print(f\"{n_rows-X.shape[0]} rows deleted :)\")\n    return X\n        \n#df = only_once_values(df)","metadata":{"papermill":{"duration":0.074401,"end_time":"2022-01-21T16:18:31.998539","exception":false,"start_time":"2022-01-21T16:18:31.924138","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:19.686726Z","iopub.execute_input":"2022-01-29T20:03:19.687392Z","iopub.status.idle":"2022-01-29T20:03:19.699121Z","shell.execute_reply.started":"2022-01-29T20:03:19.687353Z","shell.execute_reply":"2022-01-29T20:03:19.698510Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"###  couple of issues here\nWe need to replace \"Brk Comm\" items to \"BrkComm\" and find items that it's GarageYrBlt is earier than YearBuilt.","metadata":{"papermill":{"duration":0.064371,"end_time":"2022-01-21T16:18:32.126947","exception":false,"start_time":"2022-01-21T16:18:32.062576","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def Clean(df):\n    # replace \"Brk Comm\" items to \"BrkComm\"\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\n        \"Brk Cmn\": \"BrkComm\"\n    })\n    # find items that it's GarageYrBlt is earier than YearBuilt\n    #print(df[df.apply(lambda x: x['GarageYrBlt'] < x['YearBuilt'], axis = 1)][['GarageYrBlt', 'YearBuilt']] )\n    # replace corrupted items with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt > 2010, df.YearBuilt)\n    return df\n\ndf = Clean(df)","metadata":{"papermill":{"duration":0.076662,"end_time":"2022-01-21T16:18:32.267612","exception":false,"start_time":"2022-01-21T16:18:32.19095","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:19.700276Z","iopub.execute_input":"2022-01-29T20:03:19.701106Z","iopub.status.idle":"2022-01-29T20:03:19.718259Z","shell.execute_reply.started":"2022-01-29T20:03:19.701075Z","shell.execute_reply":"2022-01-29T20:03:19.717485Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Scale\nScale generally means to change the range of the values. The shape of the distribution doesn’t change. Think about how a scale model of a building has the same proportions as the original, just smaller. That’s why we say it is drawn to scale. The range is often set at 0 to 1.\n### Standardize \nStandardize generally means changing the values so that the distribution’s standard deviation equals one. Scaling is often implied.\n### Normalize \nNormalize can be used to mean either of the above things (and more!). I suggest you avoid the term normalize, because it has many definitions and is prone to creating confusion.","metadata":{"papermill":{"duration":0.063154,"end_time":"2022-01-21T16:18:32.394893","exception":false,"start_time":"2022-01-21T16:18:32.331739","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def Standalizer(X):\n    return (X - X.mean(axis=0)) / X.std(axis=0)\n\ndef Numericalscaler(X):\n    return (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))","metadata":{"papermill":{"duration":0.072312,"end_time":"2022-01-21T16:18:32.530968","exception":false,"start_time":"2022-01-21T16:18:32.458656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:19.719554Z","iopub.execute_input":"2022-01-29T20:03:19.719779Z","iopub.status.idle":"2022-01-29T20:03:19.730095Z","shell.execute_reply.started":"2022-01-29T20:03:19.719751Z","shell.execute_reply":"2022-01-29T20:03:19.729300Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Let's check for duplicated rows in our dataset.","metadata":{"papermill":{"duration":0.063554,"end_time":"2022-01-21T16:18:32.659166","exception":false,"start_time":"2022-01-21T16:18:32.595612","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df.duplicated().any() ","metadata":{"papermill":{"duration":0.102666,"end_time":"2022-01-21T16:18:32.827253","exception":false,"start_time":"2022-01-21T16:18:32.724587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:19.731412Z","iopub.execute_input":"2022-01-29T20:03:19.731642Z","iopub.status.idle":"2022-01-29T20:03:19.760717Z","shell.execute_reply.started":"2022-01-29T20:03:19.731614Z","shell.execute_reply":"2022-01-29T20:03:19.759780Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Missing Value?\nTheoretically, 25 to 30% is the maximum missing values are allowed, beyond which we might want to drop the variable from analysis.","metadata":{"papermill":{"duration":0.063731,"end_time":"2022-01-21T16:18:32.956535","exception":false,"start_time":"2022-01-21T16:18:32.892804","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def Missing_Percentage(df, drop_percentage=25):\n    X = df.copy()\n    missing_values = X.isnull().sum()\n    missing_values = missing_values[missing_values > 0].sort_values(ascending = False)\n    NAN_col = list(missing_values.to_dict().keys())\n    missing_values_data = pd.DataFrame(missing_values)\n    missing_values_data.reset_index(level=0, inplace=True)\n    missing_values_data.columns = ['Feature','Number of Missing Values']\n    missing_values_data['NA Percentage'] = (100.0 * missing_values_data['Number of Missing Values']) / X.shape[0]\n    missing_values_data['Describe'] = missing_values_data.Feature.map(data_description)\n    missing_values_data['dtype'] = [ X[name].dtype for name in missing_values_data.Feature ]\n    \n    corr_matrix = X.corr()\n    corr_dict = {}\n    for i in NAN_col:\n        if i in corr_matrix.index:\n            corr_dict[i] = np.abs(corr_matrix[i]).drop([i]).sort_values(ascending=False).index[0]\n        else:\n            corr_dict[i] = '-'\n    missing_values_data['Correlated Feature'] = missing_values_data.Feature.map(corr_dict)\n    # features to drop\n    #drop_features = missing_values_data[missing_values_data['NA Percentage'] > drop_percentage].Feature.tolist()\n    return missing_values_data.set_index(missing_values_data.Feature)\n\nmissing_df = Missing_Percentage(df, 25)\nmissing_df","metadata":{"papermill":{"duration":0.119458,"end_time":"2022-01-21T16:18:33.140162","exception":false,"start_time":"2022-01-21T16:18:33.020704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:19.761858Z","iopub.execute_input":"2022-01-29T20:03:19.762085Z","iopub.status.idle":"2022-01-29T20:03:19.803176Z","shell.execute_reply.started":"2022-01-29T20:03:19.762056Z","shell.execute_reply":"2022-01-29T20:03:19.802312Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_df.index, y=missing_df['NA Percentage']);\nplt.xlabel('Features', fontsize=15);\nplt.ylabel('Percent of missing values', fontsize=15);\nplt.title('Percent missing data by feature', fontsize=15);","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:19.804281Z","iopub.execute_input":"2022-01-29T20:03:19.804506Z","iopub.status.idle":"2022-01-29T20:03:20.340788Z","shell.execute_reply.started":"2022-01-29T20:03:19.804480Z","shell.execute_reply":"2022-01-29T20:03:20.339851Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Handle Missing Value\nHandling missing values now will make the feature engineering go more smoothly. As a start we will deal with features with null value discovered in the dataset individually.\n\n* Pool quality feature (PoolQC)   \nWe would fill all missing values with 'None' since the NAN values here simply represents that the house does not have a pool.\n\n* Miscellaneous (MiscFeature)     \nIt represents Miscellaneous feature not covered in other categories. As PoolQC feature we will fill all missing values with 'None'.\n\n* Alley access (Alley)      \nIt represents Type of alley access so we will fill all missing values with 'None'.\n\n* Fence quality (Fence)   \nWe will fill all missing values with 'None'.\n\n* Fireplace quality (FireplaceQu)    \nWe would fill all missing values with most frequently used value.\n\n* LotFrontage     \nHere we would first fill all missing values by taking the mean of the LotFrontage values of all groups having the same values of 1stFlrSF. This is because LotFrontage has a high correlation with 1stFlrSF. However, there can be cases where all the LotFrontage values corresponding to a particular 1stFlrSF value can be missing. To tackle such cases we would then fill it by using interpolate function of pandas to fill missing values linearly.\n\n* Garage (GarageType, GarageFinish, GarageQual, GarageCond)   \nNo garage? so we will fill all missing values with 'None'.\n\n* Basement (BsmtExposure, BsmtFinType2, BsmtFinType1, BsmtCond,BsmtQual)  \nNo basement? so we will fill all missing values with 'None'.\n\n* Masonry veneer (MasVnrArea, MasVnrType)  \nIt represents Type of Masonry veneer so we will fill all missing values with 'None' forMasVnrType feature and 0 for MasVnrArea feature.\n\n* Electrical system (Electrical)    \nWe would fill all missing values with most frequently used value.\n\nOtherwise, for categorical features we will use most frequently and foe numerical features\n\n##### Other ideas:\n    - replace infinity values with null values.\n    - remove rows with missing target, separate target from predictors.","metadata":{"papermill":{"duration":0.064511,"end_time":"2022-01-21T16:18:33.269896","exception":false,"start_time":"2022-01-21T16:18:33.205385","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def ImputeData(df, name):\n    from sklearn.impute import KNNImputer\n    \n    numerical_df = df.select_dtypes(include='number')\n    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n    imputer.fit(numerical_df)\n    X_transformed = imputer.transform(numerical_df)\n    df_miss = pd.DataFrame(\n        X_transformed, \n        columns = numerical_df.columns\n    )\n    #all_data[col_to_impute] = df_miss[col_to_impute]\n    return df_miss[name]\n\nImputeData(df, 'LotFrontage')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:20.342469Z","iopub.execute_input":"2022-01-29T20:03:20.342710Z","iopub.status.idle":"2022-01-29T20:03:20.434682Z","shell.execute_reply.started":"2022-01-29T20:03:20.342680Z","shell.execute_reply":"2022-01-29T20:03:20.433679Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def Impute(df):\n    X = df.copy()\n    #print(f\"Null Columns are { X.columns[ X.isnull().any() ].tolist()}\")\n    \n    # Remove rows with missing target, separate target from predictors\n    #print( df_train.SalePrice.isnull().sum() )\n    #X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n    \n    # Replace infinity values with null values\n    #print( np.isfinite(X.all()).sum() )\n    #X.replace([np.inf, -np.inf], np.nan, inplace=True)\n    \n    # MasVnrArea feature\n    X['MasVnrArea'].fillna(0, inplace=True)\n    \n    # LotFrontage feature\n    X['LotFrontage'].fillna(\n        X.groupby('1stFlrSF')['LotFrontage'].transform('mean'), \n        inplace = True\n    )\n    X['LotFrontage'].interpolate(\n        method = 'linear',\n        inplace = True\n    )\n\n    for name in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'LotFrontage', 'GarageType',\\\n                 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtExposure', 'BsmtFinType2',\\\n                 'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrType']: \n        X[name].fillna('None', inplace=True)\n    \n    for name in X.select_dtypes(include=['number']).columns:\n        # fill nan with mean \n        X[name].fillna(df.mean(axis=0)[name], inplace=True)\n        \n    for name in X.select_dtypes(include=np.object).columns:\n        # including 'FireplaceQu', 'Electrical' features\n        # fill nan with most frequency\n        most_frequently = X[name].value_counts().index[0]\n        X[name].fillna(most_frequently, inplace=True)\n        \n    # check if no null value left\n    assert(X.isnull().sum().sum() == 0)\n    #print( np.isfinite(X.all()).sum() )\n    #assert(np.isfinite(X.all()).sum() == 0) \n     \n    return X\n\ndf = Impute(df)","metadata":{"papermill":{"duration":1.174797,"end_time":"2022-01-21T16:18:34.510488","exception":false,"start_time":"2022-01-21T16:18:33.335691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:20.436786Z","iopub.execute_input":"2022-01-29T20:03:20.437368Z","iopub.status.idle":"2022-01-29T20:03:21.268844Z","shell.execute_reply.started":"2022-01-29T20:03:20.437305Z","shell.execute_reply":"2022-01-29T20:03:21.268254Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Show categorical data description","metadata":{"papermill":{"duration":0.064586,"end_time":"2022-01-21T16:18:34.640496","exception":false,"start_time":"2022-01-21T16:18:34.57591","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_desc = df.describe(include=np.object).T\ndf_desc['describe'] = df_desc.index.map(data_description)\ndf_desc['categories'] = pd.DataFrame({\n        name: ', '.join(df[name].unique().tolist()) \n        for name in df.select_dtypes(exclude='number').columns.tolist()\n    }, index=['categories']).T\ndf_desc","metadata":{"papermill":{"duration":0.166958,"end_time":"2022-01-21T16:18:34.873129","exception":false,"start_time":"2022-01-21T16:18:34.706171","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:21.269929Z","iopub.execute_input":"2022-01-29T20:03:21.270186Z","iopub.status.idle":"2022-01-29T20:03:21.358301Z","shell.execute_reply.started":"2022-01-29T20:03:21.270158Z","shell.execute_reply":"2022-01-29T20:03:21.357706Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Outliers Treatment\n\n    - Flooring and Capping: quantile-based technique.\n    - Trimming: remove and completely drop all the outliers.\n    - Replacing outliers: with the mean, median, mode, or other values. 'adviced not to use mean values'\n   ","metadata":{"papermill":{"duration":0.06715,"end_time":"2022-01-21T16:18:35.80324","exception":false,"start_time":"2022-01-21T16:18:35.73609","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Remove all rows that have outliers in at least one column    \n\n* For each column, it first computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n* It then takes the absolute Z-score because the direction does not matter, only if it is below the threshold.\n* all(axis=1) ensures that for each row, all column satisfy the constraint.\n    Finally, the result of this condition is used to index the dataframe.\n","metadata":{"papermill":{"duration":0.064604,"end_time":"2022-01-21T16:18:35.934285","exception":false,"start_time":"2022-01-21T16:18:35.869681","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from scipy import stats\nfrom time import time\n\nstandard_deviations = 3\n\n#outliers = df[ df.apply(lambda x: np.abs(x - x.mean()) / x.std() < standard_deviations).all(axis=1) ]\n#outliers = df[ ( np.abs( stats.zscore( df ) ) < standard_deviations ).all(axis=1) ]\n# drop outliers columns, first we add \n#pd.concat([df, outliers]).drop_duplicates(keep=False)\n\n# Select and remove outliers from dtata\ndef remove_outliers(df, standard_deviations=3):\n    X = df.copy()\n    z_score = stats.zscore( X.select_dtypes(include='number') )\n    return X[ ~(  np.abs(z_score)  < standard_deviations ).all(axis=1) ]\n\nremove_outliers(df, standard_deviations=3)","metadata":{"papermill":{"duration":0.129251,"end_time":"2022-01-21T16:18:36.128161","exception":false,"start_time":"2022-01-21T16:18:35.99891","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:21.359319Z","iopub.execute_input":"2022-01-29T20:03:21.359630Z","iopub.status.idle":"2022-01-29T20:03:21.405652Z","shell.execute_reply.started":"2022-01-29T20:03:21.359604Z","shell.execute_reply":"2022-01-29T20:03:21.404732Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Let's remove the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.\n\nThink this deletion is safe to do as there are only 2 points and they seem to be abnormal, possibly data error.","metadata":{}},{"cell_type":"code","source":"df[(df['OverallQual']>9) & (df['SalePrice']<220000)]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:21.406785Z","iopub.execute_input":"2022-01-29T20:03:21.407022Z","iopub.status.idle":"2022-01-29T20:03:21.431192Z","shell.execute_reply.started":"2022-01-29T20:03:21.406992Z","shell.execute_reply":"2022-01-29T20:03:21.430377Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df[(df['GrLivArea']>4000) & (df['SalePrice']<300000)]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:21.432438Z","iopub.execute_input":"2022-01-29T20:03:21.432674Z","iopub.status.idle":"2022-01-29T20:03:21.457303Z","shell.execute_reply.started":"2022-01-29T20:03:21.432645Z","shell.execute_reply":"2022-01-29T20:03:21.456707Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def delete_uninformative(df):\n    # extremely large areas for very low prices\n    X = df.copy()\n    X = X.drop(\n        X[\n            (X['OverallQual']>9) & (X['SalePrice']<220000)\n        ].index\n    )\n    X = X.drop(\n        X[\n            (X['GrLivArea']>4000) & (X['SalePrice']<300000)\n        ].index\n    )\n    return X\n\ndf = delete_uninformative(df)# dropped 2 rows from training data","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:21.458276Z","iopub.execute_input":"2022-01-29T20:03:21.459023Z","iopub.status.idle":"2022-01-29T20:03:21.474108Z","shell.execute_reply.started":"2022-01-29T20:03:21.458991Z","shell.execute_reply":"2022-01-29T20:03:21.473419Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# filtering on a column by Interquartile Range(IQR)\ndef Interquartile_Range(df):\n    iqr = pd.DataFrame({\n        'Q1': df.quantile(0.25), # first quantile,\n        'Q3': df.quantile(0.75), # second quantile,\n    })\n    iqr['IQR'] = iqr.Q3 - iqr.Q1\n    return iqr\n\n\ndef outlier_detect_new(df, name, iqr_df, whisker_width=1.5):\n    \"\"\"Remove outliers from a dataframe by column, including optional \n       whiskers, removing rows for which the column value are \n       less than Q1-1.5IQR or greater than Q3+1.5IQR.\n    Args:\n        df (`:obj:pd.DataFrame`): A pandas dataframe to subset\n        column (str): Name of the column to calculate the subset from.\n        whisker_width (float): Optional, loosen the IQR filter by a\n                               factor of `whisker_width` * IQR.\n    Returns:\n        (`:obj:pd.DataFrame`): Filtered dataframe\n    \"\"\"\n    lower_bound = iqr_df.Q1[name] - whisker_width * iqr_df.IQR[name]\n    upper_bound = iqr_df.Q3[name] + whisker_width * iqr_df.IQR[name]\n    return df[ \n        ((df[name] < lower_bound) | (df[name] > upper_bound)) \n    ]\n\ndef lower_outlier(df, name, iqr_df, whisker_width=1.5):\n    lower_bound =  iqr_df.Q1[name] - whisker_width * iqr_df.IQR[name]\n    return df[ ( df[name] < lower_bound ) ]\n\ndef upper_outlier(df, name, iqr_df, whisker_width=1.5):\n    upper_bound =  iqr_df.Q3[name] + whisker_width * iqr_df.IQR[name]\n    return df[ ( df[name] < upper_bound ) ]","metadata":{"papermill":{"duration":0.082389,"end_time":"2022-01-21T16:18:36.277178","exception":false,"start_time":"2022-01-21T16:18:36.194789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:21.475354Z","iopub.execute_input":"2022-01-29T20:03:21.475972Z","iopub.status.idle":"2022-01-29T20:03:21.485861Z","shell.execute_reply.started":"2022-01-29T20:03:21.475924Z","shell.execute_reply":"2022-01-29T20:03:21.485265Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Remove rows with missing target, separate target from predictors\ntarget = df.SalePrice\nfeaures = df.drop(['SalePrice'], axis=1)","metadata":{"papermill":{"duration":0.080268,"end_time":"2022-01-21T16:18:36.426377","exception":false,"start_time":"2022-01-21T16:18:36.346109","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:21.486978Z","iopub.execute_input":"2022-01-29T20:03:21.487355Z","iopub.status.idle":"2022-01-29T20:03:21.502936Z","shell.execute_reply.started":"2022-01-29T20:03:21.487319Z","shell.execute_reply":"2022-01-29T20:03:21.502112Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Encode the Statistical Data Type\nHere we will use one hot encoder for the features that has high cardinality columns and the features with unordered categories.\notherwise we will use label encoder. maximum cardinality in traing dataset id 25 for \"Neighborhood\" column\n* \"Cardinality\" means the number of unique values in a column\n\n\n#### Label Encoder Methods\n\n* pd.factorize()\n* np.where(series == 'yes', 1, 0)\n* pd.Categorical/astype('category'): series.astype('category').cat.codes\n* series.replace({'yes' : 1, 'no' : 0})\n* series.replace({r'^(?!yes).*$' : 0}, regex=True).astype(bool).astype(int)\n* dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})","metadata":{}},{"cell_type":"code","source":"def label_encode(df):\n    X = df.copy()\n    for name in X.select_dtypes(include=np.object):\n        X[name], _ = X[name].factorize()\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:21.504625Z","iopub.execute_input":"2022-01-29T20:03:21.504954Z","iopub.status.idle":"2022-01-29T20:03:21.514080Z","shell.execute_reply.started":"2022-01-29T20:03:21.504910Z","shell.execute_reply":"2022-01-29T20:03:21.512585Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#  Avoid OneHot for high cardinality columns\n\n#print( df.select_dtypes(include=np.object).nunique().max() )  # 25\n    \ndef Encoder(df, max_number=15 ):\n    X = df.copy()\n    #print(X.select_dtypes(include=np.object).nunique())\n    \n    #for colname in X.select_dtypes([\"category\"]):\n    #    X[colname] = X[colname].cat.codes\n    \n    for name in X.select_dtypes(include=np.object):\n        if X[name].nunique() > max_number: # and ordered features\n            X[name], _ = X[name].factorize()\n            \n        else:\n            X = pd.get_dummies(X, prefix=[f\"{name}_\"], columns=[name])\n            \n    return X\n\ndf = Encoder(df, max_number=5)\nEncoder(df, max_number=5)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:03:21.515336Z","iopub.execute_input":"2022-01-29T20:03:21.515584Z","iopub.status.idle":"2022-01-29T20:03:21.741454Z","shell.execute_reply.started":"2022-01-29T20:03:21.515556Z","shell.execute_reply":"2022-01-29T20:03:21.740700Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"##  Feature Utility Metric\nUse a metric called \"mutual information\" to compute a utility score for a feature, giving you an indication of how much potential the feature has.\nIt's a lot like correlation in that it measures a relationship between two quantities. \nThe advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships.","metadata":{"papermill":{"duration":0.066005,"end_time":"2022-01-21T16:18:36.559018","exception":false,"start_time":"2022-01-21T16:18:36.493013","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# All discrete features should now have integer dtypes\ndiscrete_features = feaures.dtypes == int","metadata":{"papermill":{"duration":0.073807,"end_time":"2022-01-21T16:18:36.702911","exception":false,"start_time":"2022-01-21T16:18:36.629104","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:21.745116Z","iopub.execute_input":"2022-01-29T20:03:21.745389Z","iopub.status.idle":"2022-01-29T20:03:21.750740Z","shell.execute_reply.started":"2022-01-29T20:03:21.745359Z","shell.execute_reply":"2022-01-29T20:03:21.749639Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def make_mi_scores(X, y):\n    # All discrete features should now have integer dtypes\n    X = label_encode(X)\n    #discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    discrete_features = (X.dtypes == int) \n    \n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(feaures, target)","metadata":{"papermill":{"duration":7.588512,"end_time":"2022-01-21T16:18:44.360739","exception":false,"start_time":"2022-01-21T16:18:36.772227","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:21.752100Z","iopub.execute_input":"2022-01-29T20:03:21.752457Z","iopub.status.idle":"2022-01-29T20:03:24.367624Z","shell.execute_reply.started":"2022-01-29T20:03:21.752424Z","shell.execute_reply":"2022-01-29T20:03:24.366714Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Here we create bar plot based on mutual information scores.","metadata":{"papermill":{"duration":0.065889,"end_time":"2022-01-21T16:18:44.493292","exception":false,"start_time":"2022-01-21T16:18:44.427403","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \nplot_mi_scores(mi_scores[:10])","metadata":{"papermill":{"duration":0.074419,"end_time":"2022-01-21T16:18:44.633654","exception":false,"start_time":"2022-01-21T16:18:44.559235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:24.368765Z","iopub.execute_input":"2022-01-29T20:03:24.368994Z","iopub.status.idle":"2022-01-29T20:03:24.715070Z","shell.execute_reply.started":"2022-01-29T20:03:24.368965Z","shell.execute_reply":"2022-01-29T20:03:24.714502Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Now we have a number of features that are highly informative. the top scoring features will usually pay-off the most during feature development, so it could be a good idea to focus efforts on those. On the other hand, training on uninformative features can lead to overfitting. So, the features with 0.0 scores we'll drop entirely.","metadata":{"papermill":{"duration":0.066305,"end_time":"2022-01-21T16:18:44.767055","exception":false,"start_time":"2022-01-21T16:18:44.70075","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def drop_uninformative(df, mi_scores, threshold=0.0):\n    return df.loc[:, mi_scores > threshold]\n\ndrop_uninformative(feaures, mi_scores, 0.0)","metadata":{"papermill":{"duration":0.07955,"end_time":"2022-01-21T16:18:44.916103","exception":false,"start_time":"2022-01-21T16:18:44.836553","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:24.716205Z","iopub.execute_input":"2022-01-29T20:03:24.716655Z","iopub.status.idle":"2022-01-29T20:03:24.746107Z","shell.execute_reply.started":"2022-01-29T20:03:24.716624Z","shell.execute_reply":"2022-01-29T20:03:24.745279Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"top_features = df.corr()[['SalePrice']].sort_values(by=['SalePrice'],ascending=False)[:20]\n\nplt.figure(figsize=(5,10));\nsns.heatmap(top_features, cmap='rainbow', annot=True, annot_kws={\"size\": 16}, vmin=-1);","metadata":{"papermill":{"duration":2.702528,"end_time":"2022-01-21T16:18:47.694364","exception":false,"start_time":"2022-01-21T16:18:44.991836","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:24.747504Z","iopub.execute_input":"2022-01-29T20:03:24.747918Z","iopub.status.idle":"2022-01-29T20:03:25.442033Z","shell.execute_reply.started":"2022-01-29T20:03:24.747881Z","shell.execute_reply":"2022-01-29T20:03:25.440473Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Creating Features\n Here we apply some strategies for creating features in Pandas.\n \n #### Creating features strategies\n* Mathematical Transforms\n* Counts\n* Building-Up and Breaking-Down Features\n* Group Transforms\n* K-means\n* PCA","metadata":{"papermill":{"duration":0.072482,"end_time":"2022-01-21T16:18:47.841031","exception":false,"start_time":"2022-01-21T16:18:47.768549","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_desc = feaures.describe(include=['number']).T\ndf_desc['describe'] = df_desc.index.map(data_description)\n\ndf_desc.dropna() # drop new columns ","metadata":{"papermill":{"duration":0.533113,"end_time":"2022-01-21T16:18:48.446376","exception":false,"start_time":"2022-01-21T16:18:47.913263","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.444778Z","iopub.execute_input":"2022-01-29T20:03:25.445286Z","iopub.status.idle":"2022-01-29T20:03:25.538177Z","shell.execute_reply.started":"2022-01-29T20:03:25.445232Z","shell.execute_reply":"2022-01-29T20:03:25.537186Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def mathematical_transforms(df):\n    X = pd.DataFrame(\n        index = df.index\n    )  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea  # create a single null value\n    X[\"Spaciousness\"] = (df['1stFlrSF'] + df['2ndFlrSF']) / df.TotRmsAbvGrd # create a single null value\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF    \n    #X['Total_Square_Feet'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['TotalBsmtSF'])\n    X['Total_Bath'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    X['Total_Porch_Area'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n    X['SqFtPerRoom'] = df['GrLivArea'] / (df['TotRmsAbvGrd'] + df['FullBath'] + df['HalfBath'] + df['KitchenAbvGr'])\n    return X\n\n#df.join(mathematical_transforms(df))#.isnull().sum().sum() # 2 null values\nmathematical_transforms(df)","metadata":{"papermill":{"duration":0.083217,"end_time":"2022-01-21T16:18:48.600433","exception":false,"start_time":"2022-01-21T16:18:48.517216","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:11:26.031252Z","iopub.execute_input":"2022-01-29T20:11:26.032116Z","iopub.status.idle":"2022-01-29T20:11:26.056411Z","shell.execute_reply.started":"2022-01-29T20:11:26.032065Z","shell.execute_reply":"2022-01-29T20:11:26.055790Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\") # deleted after using one hot\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n#df.join(interactions(df))#.isnull().sum().sum()\n#interactions(df)","metadata":{"papermill":{"duration":0.078377,"end_time":"2022-01-21T16:18:48.749838","exception":false,"start_time":"2022-01-21T16:18:48.671461","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.569100Z","iopub.execute_input":"2022-01-29T20:03:25.569915Z","iopub.status.idle":"2022-01-29T20:03:25.575533Z","shell.execute_reply.started":"2022-01-29T20:03:25.569859Z","shell.execute_reply":"2022-01-29T20:03:25.574534Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def counts(df):\n    X = pd.DataFrame(\n         index=df.index\n    )\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"3SsnPorch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n#df.join(counts(df)).isnull().sum().sum()#\ncounts(df)","metadata":{"papermill":{"duration":0.103857,"end_time":"2022-01-21T16:18:48.928718","exception":false,"start_time":"2022-01-21T16:18:48.824861","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.577971Z","iopub.execute_input":"2022-01-29T20:03:25.578263Z","iopub.status.idle":"2022-01-29T20:03:25.603760Z","shell.execute_reply.started":"2022-01-29T20:03:25.578199Z","shell.execute_reply":"2022-01-29T20:03:25.602826Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def Building_Up_Breaking_Down(df, col):\n    X = pd.DataFrame(\n        df.index\n    )\n    #X[f\"new{col}\"] = df[col].str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n#df.join(Building_Up_Breaking_Down(df, col=''))#.isnull().sum().sum()\n#Building_Up_Breaking_Down(df, col='')","metadata":{"papermill":{"duration":0.079764,"end_time":"2022-01-21T16:18:49.084278","exception":false,"start_time":"2022-01-21T16:18:49.004514","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.605186Z","iopub.execute_input":"2022-01-29T20:03:25.606121Z","iopub.status.idle":"2022-01-29T20:03:25.611989Z","shell.execute_reply.started":"2022-01-29T20:03:25.606073Z","shell.execute_reply":"2022-01-29T20:03:25.610830Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X\n\n#df.join(group_transforms(df))#.isnull().sum().sum()\ngroup_transforms(df)","metadata":{"papermill":{"duration":0.079619,"end_time":"2022-01-21T16:18:49.235914","exception":false,"start_time":"2022-01-21T16:18:49.156295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.613407Z","iopub.execute_input":"2022-01-29T20:03:25.614301Z","iopub.status.idle":"2022-01-29T20:03:25.641334Z","shell.execute_reply.started":"2022-01-29T20:03:25.614256Z","shell.execute_reply":"2022-01-29T20:03:25.640159Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"#### KMeans\nAn nsupervised algorithm we used to create features was k-means clustering. We saw that you could either use the cluster labels as a feature (a column with 0, 1, 2, ...) or you could use the distance of the observations to each cluster. We saw how these features can sometimes be effective at untangling complicated spatial relationships.","metadata":{"papermill":{"duration":0.072802,"end_time":"2022-01-21T16:18:49.379632","exception":false,"start_time":"2022-01-21T16:18:49.30683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"GrLivArea\",\n]","metadata":{"papermill":{"duration":0.081663,"end_time":"2022-01-21T16:18:49.535759","exception":false,"start_time":"2022-01-21T16:18:49.454096","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.643549Z","iopub.execute_input":"2022-01-29T20:03:25.643978Z","iopub.status.idle":"2022-01-29T20:03:25.653266Z","shell.execute_reply.started":"2022-01-29T20:03:25.643938Z","shell.execute_reply":"2022-01-29T20:03:25.652045Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = Standalizer(X_scaled)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame(index=X.index)\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\ndf.join(cluster_labels(df, cluster_features)).isnull().sum().sum()\ncluster_labels(df, cluster_features)","metadata":{"papermill":{"duration":0.07979,"end_time":"2022-01-21T16:18:49.686943","exception":false,"start_time":"2022-01-21T16:18:49.607153","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:03:25.656166Z","iopub.execute_input":"2022-01-29T20:03:25.657259Z","iopub.status.idle":"2022-01-29T20:04:21.927141Z","shell.execute_reply.started":"2022-01-29T20:03:25.657192Z","shell.execute_reply":"2022-01-29T20:04:21.926307Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = Standalizer(X_scaled)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    # Label features and join to dataset\n    X_cd = kmeans.fit_transform(X_scaled)\n    X_cd = pd.DataFrame(\n        X_cd, \n        columns = [f\"Centroid_{i}\" for i in range(X_cd.shape[1])],\n        index = X.index\n    )\n    return X_cd\n\ndf.join(cluster_distance(df, cluster_features)).isnull().isnull().sum().sum()\ncluster_distance(df, cluster_features)","metadata":{"papermill":{"duration":0.086593,"end_time":"2022-01-21T16:18:49.844097","exception":false,"start_time":"2022-01-21T16:18:49.757504","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:04:21.928807Z","iopub.execute_input":"2022-01-29T20:04:21.929400Z","iopub.status.idle":"2022-01-29T20:05:19.831002Z","shell.execute_reply.started":"2022-01-29T20:04:21.929355Z","shell.execute_reply":"2022-01-29T20:05:19.830185Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"#### PCA\nAn unsupervised model we used for feature creation. It could be used to decompose the variational structure in the data. The PCA algorithm gave us loadings which described each component of variation, and also the components which were the transformed datapoints. The loadings can suggest features to create and the components we can use as features directly.","metadata":{"papermill":{"duration":0.07091,"end_time":"2022-01-21T16:18:49.986704","exception":false,"start_time":"2022-01-21T16:18:49.915794","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","metadata":{"papermill":{"duration":0.077339,"end_time":"2022-01-21T16:18:50.134777","exception":false,"start_time":"2022-01-21T16:18:50.057438","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:05:19.832657Z","iopub.execute_input":"2022-01-29T20:05:19.833231Z","iopub.status.idle":"2022-01-29T20:05:19.837653Z","shell.execute_reply.started":"2022-01-29T20:05:19.833172Z","shell.execute_reply":"2022-01-29T20:05:19.837012Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def pca_components(df, features, standardize=True):\n    X = df.copy()\n    X = X.loc[:, features]\n    # Standardize\n    if standardize: Standalizer(X)\n    # Create principal components\n    pca_model = PCA()\n    X_pca = pca_model.fit_transform(X)\n    \n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(\n        X_pca, \n        columns = component_names,\n        index = X.index\n    )\n    \n    # Create loadings\n    loadings = pd.DataFrame(\n        pca_model.components_.T,  # transpose the matrix of loadings\n        columns = component_names,  # so the columns are the principal components\n        index = X.columns,  # and the rows are the original features\n    )\n    \n    return pca_model, X_pca, loadings\n\npca_model, X_pca, loadings = pca_components(df, pca_features)\nX_pca","metadata":{"papermill":{"duration":0.091585,"end_time":"2022-01-21T16:18:50.296866","exception":false,"start_time":"2022-01-21T16:18:50.205281","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:05:19.839025Z","iopub.execute_input":"2022-01-29T20:05:19.839506Z","iopub.status.idle":"2022-01-29T20:05:19.880342Z","shell.execute_reply.started":"2022-01-29T20:05:19.839473Z","shell.execute_reply":"2022-01-29T20:05:19.879677Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    fig.set(figwidth=8, dpi=100)\n    \n    n = pca.n_components_\n    print(f\"pca n components: {pca.n_components_}\")\n    grid = np.arange(1, n + 1)\n    \n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    print(f\"pca  explained variance ratio: {pca.explained_variance_ratio_}\")\n    \n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    #return axs\n\nplot_variance(pca_model, width=8, dpi=100)","metadata":{"papermill":{"duration":0.081956,"end_time":"2022-01-21T16:18:50.449216","exception":false,"start_time":"2022-01-21T16:18:50.36726","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:05:19.884642Z","iopub.execute_input":"2022-01-29T20:05:19.886719Z","iopub.status.idle":"2022-01-29T20:05:20.516577Z","shell.execute_reply.started":"2022-01-29T20:05:19.886668Z","shell.execute_reply":"2022-01-29T20:05:20.515912Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## some ideas for other transforms you could explore:\n","metadata":{"papermill":{"duration":0.07121,"end_time":"2022-01-21T16:18:50.590929","exception":false,"start_time":"2022-01-21T16:18:50.519719","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Interactions between the quality Qual and condition Cond features. \n    OverallQual, for instance, was a high-scoring feature. \n    You could try combining it with OverallCond by converting both to integer type \n    and taking a product.","metadata":{"papermill":{"duration":0.070297,"end_time":"2022-01-21T16:18:50.731615","exception":false,"start_time":"2022-01-21T16:18:50.661318","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.071174,"end_time":"2022-01-21T16:18:50.876223","exception":false,"start_time":"2022-01-21T16:18:50.805049","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Square roots of area features. This would convert units of square feet to just feet.","metadata":{"papermill":{"duration":0.069842,"end_time":"2022-01-21T16:18:51.018549","exception":false,"start_time":"2022-01-21T16:18:50.948707","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Interactions between numeric and categorical features \nthat describe the same thing. \nYou could look at interactions between BsmtQual and TotalBsmtSF, for instance.","metadata":{"papermill":{"duration":0.071654,"end_time":"2022-01-21T16:18:52.300271","exception":false,"start_time":"2022-01-21T16:18:52.228617","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.071156,"end_time":"2022-01-21T16:18:52.4416","exception":false,"start_time":"2022-01-21T16:18:52.370444","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Other group statistics in Neighboorhood.\nWe did the median of GrLivArea. Looking at mean,\nstd, or count could be interesting. You could also try combining the group statistics with\nother features. Maybe the difference of GrLivArea and the median is important?","metadata":{"papermill":{"duration":0.071564,"end_time":"2022-01-21T16:18:52.584692","exception":false,"start_time":"2022-01-21T16:18:52.513128","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.069708,"end_time":"2022-01-21T16:18:52.725768","exception":false,"start_time":"2022-01-21T16:18:52.65606","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantile binning\n\n    The only 2-quantile is called the median\n    The 3-quantiles are called tertiles or terciles → T\n    The 4-quantiles are called quartiles → Q; the difference between upper and lower quartiles is also called the interquartile range, midspread or middle fifty → IQR = Q3 −  Q1\n    The 5-quantiles are called quintiles → QU\n    The 6-quantiles are called sextiles → S\n    The 7-quantiles are called septiles\n    The 8-quantiles are called octiles\n    The 10-quantiles are called deciles → D\n    The 12-quantiles are called duo-deciles or dodeciles\n    The 16-quantiles are called hexadeciles → H\n    The 20-quantiles are called ventiles, vigintiles, or demi-deciles → V\n    The 100-quantiles are called percentiles → P\n    The 1000-quantiles have been called permilles or milliles, but these are rare and largely obsolete","metadata":{"papermill":{"duration":0.073421,"end_time":"2022-01-21T16:18:52.86983","exception":false,"start_time":"2022-01-21T16:18:52.796409","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.069913,"end_time":"2022-01-21T16:18:53.011392","exception":false,"start_time":"2022-01-21T16:18:52.941479","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing Pipline\n\nBefore we can do any feature engineering, we need to preprocess the data to get it in a form suitable for analysis. we'll need to:\n\n    Load: the data from CSV files\n    Clean: the data to fix any errors or inconsistencies\n    Encode: the statistical data type (numeric, categorical)\n    Impute: any missing values\n    \n After reading the CSV file, we'll apply three preprocessing steps, clean, encode, and impute, and then create the data splits: one (df_train) for training the model, and one (df_test) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{"papermill":{"duration":0.072414,"end_time":"2022-01-21T16:18:53.158542","exception":false,"start_time":"2022-01-21T16:18:53.086128","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def Data():\n    # Read the data\n    df_train = pd.read_csv(\"../input/train.csv\", index_col=0)\n    df_test = pd.read_csv(\"../input/test.csv\", index_col=0)    \n    #df_train = only_once_values(df_train) # apply this in traing dataset ONLY\n    \n    # delete_uninformative\n    df_train = delete_uninformative(df_train)\n    print(f\"concatenate Done: {df_train.shape}\")\n    \n    target = df_train.pop(\"SalePrice\")\n    \n    df = pd.concat([df_train, df_test])\n    print(f\"concatenate Done: {df.shape}\")\n\n    # Preprocessing\n    df = Clean(df)\n    print(f\"Clean Done: {df.shape}\")\n    \n    df = Impute(df)\n    print(f\"Impute Done: {df.shape}\")\n    \n    df = Encoder(df, max_number=5) \n    print(f\"Encoder Done: {df.shape}\")\n    \n    # split data\n    df_train = df.loc[df_train.index, :]\n    df_train['SalePrice'] = target\n    df_test = df.loc[df_test.index, :]\n    print(df_train.shape, df_test.shape)\n    \n    return df_train, df_test\n\n#df_train, df_test = tqdm(Data())\n#print('Null Values: ', df_train.isnull().sum().sum(), df_test.isnull().sum().sum())","metadata":{"papermill":{"duration":0.081659,"end_time":"2022-01-21T16:18:53.310364","exception":false,"start_time":"2022-01-21T16:18:53.228705","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:05:20.517545Z","iopub.execute_input":"2022-01-29T20:05:20.517882Z","iopub.status.idle":"2022-01-29T20:05:20.524567Z","shell.execute_reply.started":"2022-01-29T20:05:20.517853Z","shell.execute_reply":"2022-01-29T20:05:20.523737Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Create Features Pipline\nNow we'll start developing our feature set.\n\nTo make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set.","metadata":{"papermill":{"duration":0.075185,"end_time":"2022-01-21T16:18:53.456599","exception":false,"start_time":"2022-01-21T16:18:53.381414","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_features(df, df_test):\n    X = df.copy()\n    target = X.pop(\"SalePrice\")\n    \n    mi_scores = make_mi_scores(X, target)\n    \n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    X_test = df_test.copy()\n    X = pd.concat([X, X_test])\n    print(f\"concatenate Done: {X.shape}\")\n    X_new = pd.DataFrame({}, index = X.index)\n    \n    #  Mutual Information\n    X = drop_uninformative(X, mi_scores, 0.0)\n    print(f\"drop_uninformative Done: {X.shape}\")\n    print('Null Values: ', X.isnull().sum().sum())\n\n    # Transformations\n    X_new = X_new.join(mathematical_transforms(X))\n    print(f\"mathematical_transforms Done: {X.shape}\")\n    \n    #X = X.join(interactions(X))\n    #print(f\"interactions Done: {X.shape}\")\n    \n    X_new = X_new.join(counts(X))\n    print(f\"counts Done: {X.shape}\")\n    \n    #X_new = X_new.join(Building_Up_Breaking_Down(X))\n    #print(f\"Building_Up_Breaking_Down Done: {X.shape}\")\n\n    X_new = X_new.join(group_transforms(X))\n    print(f\"group_transforms Done: {X.shape}\")\n    \n    # Clustering\n    X_new = X_new.join(cluster_labels(X, cluster_features, n_clusters=20))\n    print(f\"cluster_labels Done: {X.shape}\")\n\n    X_new = X_new.join(cluster_distance(X, cluster_features, n_clusters=20))\n    print(f\"cluster_distance Done: {X.shape}\")\n\n    # PCA\n    X_new = X_new.join(pca_components(X, pca_features)[1])\n    print(f\"pca_components Done: {X.shape}\")\n\n    #X_new = X_new.join(indicate_outliers(X))\n    \n    X = X.join(X_new)\n    \n    X = Encoder(X, 15)\n    print(f\"Encoder Done: {X.shape}\")\n        \n    # Scale\n    #X = pd.DataFrame(MinMaxScaler().fit_transform(X), index=X.index, columns=X.columns)\n    #print(f\"scaler Done: {X.shape}\")\n    \n    # ----- Reform splits -----\n    X_test = X.loc[df_test.index, :]\n    X.drop(X_test.index, inplace=True)\n    print(f\"drop_uninformative 2 Done: {X.shape}\")\n    \n    X = remove_outliers(X, standard_deviations=1)\n    target = target[X.index]\n    \n    mi_scores = make_mi_scores(X, target)\n    X = drop_uninformative(X, mi_scores, 0.0)\n    X_test = X_test[X.columns]\n    \n    return X, target, X_test\n\ndf_train, df_test = tqdm(Data())\nprint('Final shapes: ', df_train.shape, df_test.shape)\nprint('Null Values: ', df_train.isnull().sum().sum(), df_test.isnull().sum().sum())\n\nfeatures, target, df_test = tqdm(create_features(df_train, df_test))\nprint('Final shapes: ', features.shape, df_test.shape)\nprint('Null Values: ', features.isnull().sum().sum(), df_test.isnull().sum().sum())","metadata":{"papermill":{"duration":0.090738,"end_time":"2022-01-21T16:18:53.622864","exception":false,"start_time":"2022-01-21T16:18:53.532126","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:57:23.409591Z","iopub.execute_input":"2022-01-29T20:57:23.409886Z","iopub.status.idle":"2022-01-29T20:59:05.203619Z","shell.execute_reply.started":"2022-01-29T20:57:23.409858Z","shell.execute_reply":"2022-01-29T20:59:05.202848Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"## Establish Baseline\n\nFinally, let's establish a baseline score to judge our feature engineering against.\n\nThis function will compute the cross-validated RMSLE score for a feature set. We've used XGBoost for our model, but you might want to experiment with other models.","metadata":{"papermill":{"duration":0.070231,"end_time":"2022-01-21T16:18:53.764377","exception":false,"start_time":"2022-01-21T16:18:53.694146","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Divide train dataset to train/validate","metadata":{"papermill":{"duration":0.074541,"end_time":"2022-01-21T16:18:54.066826","exception":false,"start_time":"2022-01-21T16:18:53.992285","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_train, df_test = tqdm(Data())\nfeatures, target, df_test = tqdm(create_features(df_train, df_test))\n\ntrain_features, validate_features, train_target, validate_target = train_test_split(\n    features, \n    target, \n    test_size=0.25, \n    random_state=42\n)","metadata":{"papermill":{"duration":31.670614,"end_time":"2022-01-21T16:19:25.810238","exception":false,"start_time":"2022-01-21T16:18:54.139624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T20:59:05.204998Z","iopub.execute_input":"2022-01-29T20:59:05.205263Z","iopub.status.idle":"2022-01-29T21:00:48.620414Z","shell.execute_reply.started":"2022-01-29T20:59:05.205225Z","shell.execute_reply":"2022-01-29T21:00:48.619476Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning\n\nwe do here some hyperparameter tuning with GradientBoostingRegressor.","metadata":{"papermill":{"duration":0.074452,"end_time":"2022-01-21T16:19:25.96009","exception":false,"start_time":"2022-01-21T16:19:25.885638","status":"completed"},"tags":[]}},{"cell_type":"code","source":"param_grid = {\n    #'fit_intercept':[True,False],\n    'n_estimators': [100, 200, 300, 400, 500], \n    'learning_rate': [0.01, 0.05, 0.09, 0.1, 0.2, 0.9],\n}\nmodel_param_mod = GridSearchCV(\n    estimator = GradientBoostingRegressor(),\n    param_grid = param_grid, \n    n_jobs = -1\n)\n\n#model_param_mod.fit(train_features, train_target)\n#print(model_param_mod.best_params_)","metadata":{"papermill":{"duration":0.087902,"end_time":"2022-01-21T16:19:26.122347","exception":false,"start_time":"2022-01-21T16:19:26.034445","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T21:00:48.622011Z","iopub.execute_input":"2022-01-29T21:00:48.622442Z","iopub.status.idle":"2022-01-29T21:00:48.629535Z","shell.execute_reply.started":"2022-01-29T21:00:48.622394Z","shell.execute_reply":"2022-01-29T21:00:48.628606Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def basic_model(X1, y1, X2, y2):\n    # Define model\n    #model = XGBRegressor(n_estimators=200, learning_rate=0.09)\n    #model = RandomForestRegressor(bootstrap=False, max_depth=60, max_features='sqrt',n_estimators=200)\n    #model = BaggingRegressor()\n    #model = BayesianRidge()\n    #model = Lasso(alpha=100)\n    #model = ExtraTreesRegressor(n_estimators=200)\n    model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05)\n    \n    model.fit(X1, y1)\n    pred = model.predict(X2)\n    print(f\"MAE: {mean_absolute_error(y2, pred):.2f}\")\n    return model\n\nmodel = basic_model(train_features, train_target, validate_features, validate_target)","metadata":{"papermill":{"duration":5.238442,"end_time":"2022-01-21T16:19:31.437123","exception":false,"start_time":"2022-01-21T16:19:26.198681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T21:00:48.632863Z","iopub.execute_input":"2022-01-29T21:00:48.634086Z","iopub.status.idle":"2022-01-29T21:00:53.740693Z","shell.execute_reply.started":"2022-01-29T21:00:48.634026Z","shell.execute_reply":"2022-01-29T21:00:53.739772Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"validate_target_predictions = model.predict(validate_features).flatten()\nprint('MAE:', mean_absolute_error(validate_target, validate_target_predictions))\nerror = validate_target, validate_target_predictions","metadata":{"papermill":{"duration":0.092089,"end_time":"2022-01-21T16:19:31.604551","exception":false,"start_time":"2022-01-21T16:19:31.512462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T21:00:53.742066Z","iopub.execute_input":"2022-01-29T21:00:53.742604Z","iopub.status.idle":"2022-01-29T21:00:53.757011Z","shell.execute_reply.started":"2022-01-29T21:00:53.742562Z","shell.execute_reply":"2022-01-29T21:00:53.755634Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"#### regresion between true and predicted prices","metadata":{"papermill":{"duration":0.075116,"end_time":"2022-01-21T16:19:31.754925","exception":false,"start_time":"2022-01-21T16:19:31.679809","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nplt.scatter(validate_target, validate_target_predictions)\nplt.xlabel('True Values [SalePrice]')\nplt.ylabel('Predictions [SalePrice]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n## Perfect predictions\nplt.plot(validate_target, validate_target,'r');","metadata":{"papermill":{"duration":0.582553,"end_time":"2022-01-21T16:19:32.417066","exception":false,"start_time":"2022-01-21T16:19:31.834513","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T21:00:53.758404Z","iopub.execute_input":"2022-01-29T21:00:53.758857Z","iopub.status.idle":"2022-01-29T21:00:54.135961Z","shell.execute_reply.started":"2022-01-29T21:00:53.758808Z","shell.execute_reply":"2022-01-29T21:00:54.134949Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"#### It looks like our model predicts reasonably well. Let’s take a look at the error distribution.","metadata":{"papermill":{"duration":0.076093,"end_time":"2022-01-21T16:19:32.570326","exception":false,"start_time":"2022-01-21T16:19:32.494233","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(14, 12))\nplt.hist(error, bins = 50)\nplt.xlabel(\"Prediction Error [SalePrice]\")\n_ = plt.ylabel(\"Count\")","metadata":{"papermill":{"duration":0.771615,"end_time":"2022-01-21T16:19:33.418787","exception":false,"start_time":"2022-01-21T16:19:32.647172","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T21:00:54.137201Z","iopub.execute_input":"2022-01-29T21:00:54.137439Z","iopub.status.idle":"2022-01-29T21:00:54.669548Z","shell.execute_reply.started":"2022-01-29T21:00:54.137412Z","shell.execute_reply":"2022-01-29T21:00:54.668535Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def Save_Submit(model, df):\n    output = pd.DataFrame({\n        'Id': df.index,\n        'SalePrice': model.predict(df).flatten()\n    })\n    output.to_csv('submission.csv', index=False)\n    print(\"Saved!\")\n    \nmodel = GradientBoostingRegressor(learning_rate=0.05, n_estimators=300).fit(features, target)\nSave_Submit(model, df_test)","metadata":{"papermill":{"duration":7.010168,"end_time":"2022-01-21T16:19:40.506285","exception":false,"start_time":"2022-01-21T16:19:33.496117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T21:00:54.671276Z","iopub.execute_input":"2022-01-29T21:00:54.671623Z","iopub.status.idle":"2022-01-29T21:01:01.651516Z","shell.execute_reply.started":"2022-01-29T21:00:54.671579Z","shell.execute_reply":"2022-01-29T21:01:01.650072Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}